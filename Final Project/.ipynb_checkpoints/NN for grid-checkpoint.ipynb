{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5befd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea81c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'predicted_stations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2472\\88957521.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEV_Stations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predicted_stations.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mEV_Grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predicted_grid.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predicted_stations.csv'"
     ]
    }
   ],
   "source": [
    "EV_Stations = pd.DataFrame(pd.read_csv('predicted_stations.csv'))\n",
    "EV_Grid = pd.DataFrame(pd.read_csv('predicted_grid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c475c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_Stations = EV_Stations.drop(columns='Unnamed: 0')\n",
    "EV_Stations = EV_Stations.loc[(EV_Stations['Year'] >= 2010) & (EV_Stations['Year'] <= 2022)]\n",
    "EV_Stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213701ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_Vehicles = pd.DataFrame(pd.read_csv('predicted_vehicles.csv'))\n",
    "EV_Vehicles = EV_Vehicles.drop(columns='Unnamed: 0')\n",
    "EV_Vehicles = EV_Vehicles.loc[(EV_Vehicles['Year'] >= 2010) & (EV_Vehicles['Year'] <= 2022)]\n",
    "EV_Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_Grid = EV_Grid.loc[(EV_Grid['Year'] >= 2010) & (EV_Grid['Year'] <= 2022)]\n",
    "EV_Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b0f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# create a copy of the ev_grid array\n",
    "ev_grid_scaled = EV_Grid.copy()\n",
    "\n",
    "# select the predicted column\n",
    "predicted_column = ev_grid_scaled['Predicted_MWH']\n",
    "\n",
    "# scale the ev_grid value for all rows except the last row\n",
    "predicted_column[:-1] = predicted_column[:-1] / math.exp(8)\n",
    "\n",
    "# multiply the last row by e^7\n",
    "#predicted_column[-1] = predicted_column[-1] * math.exp(7)\n",
    "last_row = ev_grid_scaled.tail(1)\n",
    "last_row = last_row / math.exp(8)\n",
    "# change the last row of the predicted column\n",
    "ev_grid_scaled.iloc[-1, ev_grid_scaled.columns.get_loc('Predicted_MWH')] = last_row\n",
    "# update the predicted column in the ev_grid_scaled array\n",
    "ev_grid_scaled['Predicted_MWH'] = predicted_column\n",
    "\n",
    "\n",
    "ev_grid_scaled['Predicted_MWH'] = ev_grid_scaled['Predicted_MWH'].astype(int)\n",
    "EV_Grid=ev_grid_scaled\n",
    "\n",
    "EV_Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EV_Vehicles.shape)\n",
    "print(EV_Stations.shape)\n",
    "print(EV_Grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f76d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into X (features) and y (target)\n",
    "df = pd.merge(EV_Stations, EV_Vehicles, on='Year')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ffe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, EV_Grid, on = 'Year')\n",
    "df['Total_Sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412746b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['Total_Installed', 'Total_Sold', 'Year']])\n",
    "y = np.array(df['Predicted_MWH'])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "num_inputs = 3\n",
    "num_outputs = 1\n",
    "\n",
    "hidden_size = [32, 64]\n",
    "\n",
    "model = nn.Sequential(\n",
    "nn.Linear(num_inputs, hidden_size[0]),\n",
    "nn.ReLU(),\n",
    "nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "nn.ReLU(),\n",
    "nn.Linear(hidden_size[1], num_outputs),\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create an instance of the MinMaxScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "#scaler_y = MinMaxScaler()\n",
    "\n",
    "y=y.reshape(-1,1)\n",
    "\n",
    "# fit the scaler to the data\n",
    "scaler_x.fit(X)\n",
    "#scaler_y.fit(y)\n",
    "\n",
    "# transform the data\n",
    "X = scaler_x.transform(X)\n",
    "#y = scaler_y.transform(y)\n",
    "\n",
    "# normalize and scale the data\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "y_mean = y.mean()\n",
    "y_std = y.std()\n",
    "\n",
    "X = (X - X_mean) / X_std\n",
    "y = (y - y_mean) / y_std\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# convert the data to tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "y_test = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print(model)\n",
    "# train the model\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print training and testing accuracy\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model(X_test)\n",
    "            test_loss = criterion(y_pred_test, y_test)\n",
    "            print(f'Epoch {epoch+1}/100: Testing loss = {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f644f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data point\n",
    "X_new = pd.DataFrame({'Year': [2022], 'Total_Installed': [264.0], 'Total_Sold': [526.0]})\n",
    "\n",
    "# convert the data to a tensor\n",
    "X_new = torch.from_numpy(X_new.values).float()\n",
    "\n",
    "# normalize and scale the data\n",
    "#X_new = (X_new - X_mean) / X_std\n",
    "\n",
    "# make predictions using the model\n",
    "y_pred = model(X_new)\n",
    "\n",
    "# de-normalize the prediction\n",
    "y_pred = abs(y_pred.item())\n",
    "print(f'Predicted grid power for 2022 with 264 EV stations and EV Vehicles with 526: {abs(y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred*math.exp(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class PowerGridPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)       \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = PowerGridPredictor(input_size=3,hidden_size = [32, 64], output_size=1)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# Normalize and scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "scaler_x.fit(X)\n",
    "scaler_y.fit(y)\n",
    "\n",
    "X = scaler_x.transform(X)\n",
    "y = scaler_y.transform(y)\n",
    "\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "y_mean = y.mean()\n",
    "y_std = y.std()\n",
    "\n",
    "X = (X - X_mean) / X_std\n",
    "y = (y - y_mean) / y_std\n",
    "\n",
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Convert the data to tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "        # forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print training and testing accuracy\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model(X_test)\n",
    "            test_loss = criterion(y_pred_test, y_test)\n",
    "            print(f'Epoch {epoch+1}/3000: Testing loss = {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data point\n",
    "X_new = pd.DataFrame({'Year': [2022], 'Total_Installed': [264.0], 'Total_Sold': [526.0]})\n",
    "\n",
    "# convert the data to a tensor\n",
    "X_new = torch.from_numpy(X_new.values).float()\n",
    "\n",
    "# normalize and scale the data\n",
    "#X_new = (X_new - X_mean) / X_std\n",
    "\n",
    "# make predictions using the model\n",
    "y_pred = model(X_new)\n",
    "\n",
    "# de-normalize the prediction\n",
    "y_pred = abs(y_pred.item())\n",
    "print(f'Predicted grid power for 2022 with 264 EV stations and EV Vehicles with 526: {abs(y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred*math.exp(4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Define the year and number of stations lists\n",
    "year = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033]\n",
    "stations = [7.0, 57.0, 38.0, 16.0, 38.0, 65.0, 81.0, 97.0, 106.0, 174.0, 206.0, 683.0, 264.0, 238.11287478,256.39153439, 274.670194,   292.94885362, 311.22751323, 329.50617284, 347.78483245, 366.06349206, 384.34215168, 402.62081129,420.8994709]\n",
    "vehicles = [3.0, 13.0, 14.0, 19.0, 17.0, 32.0, 38.0, 64.0, 100.0, 147.0, 174.0, 300.0, 526.0, 718.75484848, 959.24172783, 1249.2700119, 1593.4611702, 1996.43671417, 2462.8181076, 2997.22684669, 3604.28442192, 4288.6123085, 5054.83200073, 5907.56498718]\n",
    "\n",
    "# Create empty lists to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# Round up or down depending on the decimal place\n",
    "stations = [math.ceil(n) if n % 1 >= 0.5 else math.floor(n) for n in stations]\n",
    "\n",
    "# Iterate over the year and number of stations lists\n",
    "for i in range(len(year)):\n",
    "    # Create a new data point\n",
    "    X_new = pd.DataFrame({'Year': [year[i]], 'Total_Installed': [stations[i]],'Total_Sold': [vehicles[i]]})\n",
    "\n",
    "    # Convert the data to a tensor\n",
    "    X_new = torch.from_numpy(X_new.values).float()\n",
    "\n",
    "    # Normalize and scale the data\n",
    "    #X_new = (X_new - X_mean) / X_std\n",
    "\n",
    "    # Make predictions using the model\n",
    "    y_pred = model(X_new)\n",
    "\n",
    "    # Multiply the prediction by e^4\n",
    "    result = y_pred * math.exp(4.5)\n",
    "    \n",
    "    # Add the prediction to the list\n",
    "    predictions.append(abs(result.item()))\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d93eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c43bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the list of values to a NumPy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Shift the values by exp(8)\n",
    "predictions = predictions * np.exp(8)\n",
    "\n",
    "print(predictions)  # prints [2980.95799, 2981.95799, 2982.95799, 2983.95799, 2984.95799]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "df = pd.DataFrame(pd.read_csv('annual_generation_WA.csv'))\n",
    "\n",
    "x= np.arange(32) + 1990\n",
    "\n",
    "y = df.Total.values\n",
    "\n",
    "n = np.size(x)\n",
    "\n",
    "fig=plt.figure(figsize = (15,6))\n",
    "plt.title('GGrid Capacity vs Predicted Grid vs EV Power')\n",
    "plt.plot(x,y, '-b', label='Grid')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Capacity in MWH')\n",
    "\n",
    "EV_Grid = pd.DataFrame(pd.read_csv('predicted_grid.csv'))\n",
    "EV_Grid = EV_Grid.loc[(EV_Grid['Year'] >= 2021) & (EV_Grid['Year'] <= 2033)]\n",
    "EV_Grid\n",
    "\n",
    "x= EV_Grid.Year.values\n",
    "y = EV_Grid.Predicted_MWH.values\n",
    "plt.plot(x,y, '--r', label='Predicted Grid')\n",
    "plt.plot(year,predictions, label='Predicted EV Power')\n",
    "plt.legend()\n",
    "fig.savefig('Grid Capacity vs Predicted Grid vs EV Power.jpg', bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(13, input_dim=3, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(26, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='Adagrad')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aca87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "#loss, mse = model.evaluate(X_test, y_test)\n",
    "#print(\"Test loss:\", loss)\n",
    "#print(\"Test MSE:\", mse)\n",
    "\n",
    "# Define the input features for the new data\n",
    "new_data = [[2023, 2640, 5260]]  # year, number of stations, number of vehicles\n",
    "print(new_data)\n",
    "# Use the model to make predictions on the new data\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(predictions*math.exp(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Define the year and number of stations lists\n",
    "year = [2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033]\n",
    "stations = [238.11287478, 256.39153439, 274.670194, 292.94885362, 311.22751323,329.50617284,347.78483245,366.06349206,384.34215168,402.62081129,420.8994709]\n",
    "vehicles = [359, 392, 425, 458, 491, 524, 557, 590, 623, 656, 689]\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# Round up or down depending on the decimal place\n",
    "stations = [math.ceil(n) if n % 1 >= 0.5 else math.floor(n) for n in stations]\n",
    "\n",
    "import itertools\n",
    "data = [[y, s, v] for y, s, v in itertools.zip_longest(year, stations, vehicles, fillvalue=None)]\n",
    "data[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a34bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = []\n",
    "#data_array = np.array(data)\n",
    "\n",
    "# Define the input features for the new data\n",
    "#for i in range(len(data_array[:,0])):\n",
    "#    new_data = [data[i:i+1]]  # year, number of stations, number of vehicles\n",
    "\n",
    "    # Use the model to make predictions on the new data\n",
    "#    predictions = model.predict(abs(new_data)*math.exp(5))\n",
    "\n",
    "# Print the predictions\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e941f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
